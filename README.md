# ğŸ¤– ML Benchmarking System
**BCD-7213 â€“ MinerÃ­a de Datos Avanzada Â· Universidad LEAD Â· I Cuatrimestre 2026**

> Melany RamÃ­rez Â· Jason Barrantes Â· Junior RamÃ­rez  
> Dr. Juan Murillo-Morera

---

## ğŸ“‹ DescripciÃ³n

Sistema completo de benchmarking de modelos de Machine Learning con interfaz grÃ¡fica en Streamlit. 
Soporta clasificaciÃ³n, regresiÃ³n y series de tiempo con validaciÃ³n cruzada K-Fold, AUC-ROC, ajuste 
de threshold y manejo de clases desbalanceadas.

## ğŸ—ï¸ Estructura del Proyecto

```
mlbenchmark/
â”œâ”€â”€ mlbenchmark/                  â† Paquete Python principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py          â† DivisiÃ³n, escalado, codificaciÃ³n
â”‚   â”œâ”€â”€ balancing.py              â† SMOTE, under-sampling, hÃ­brido
â”‚   â”œâ”€â”€ metrics.py                â† MÃ©tricas de clasificaciÃ³n/regresiÃ³n/TS
â”‚   â”œâ”€â”€ validation.py             â† K-Fold, Stratified K-Fold
â”‚   â”œâ”€â”€ threshold.py              â† Ajuste y optimizaciÃ³n de threshold
â”‚   â”œâ”€â”€ models_classification.py  â† Modelos de clasificaciÃ³n
â”‚   â”œâ”€â”€ models_regression.py      â† Modelos de regresiÃ³n
â”‚   â”œâ”€â”€ models_timeseries.py      â† ARIMA, Holt-Winters, LSTM
â”‚   â””â”€â”€ benchmarking.py           â† Orquestador principal
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py          â† Interfaz grÃ¡fica Streamlit
â”œâ”€â”€ tests/                        â† Pruebas unitarias
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n

```bash
# Clonar o descomprimir el proyecto
cd mlbenchmark

# Instalar dependencias
pip install -r requirements.txt

# Instalar el paquete en modo desarrollo
pip install -e .
```

## â–¶ï¸ EjecuciÃ³n

### AplicaciÃ³n Streamlit (GUI)
```bash
streamlit run app/streamlit_app.py
```
Abre tu navegador en `http://localhost:8501`

### Uso como paquete Python
```python
from mlbenchmark.benchmarking import run_benchmark

# ClasificaciÃ³n con SMOTE y K-Fold=5
result = run_benchmark(
    problem_type="classification",
    X=X.values,
    y=y.values,
    test_size=0.3,
    cv_folds=5,
    threshold=0.5,
    balancing_technique="smote",
    scale=True,
)

print(result["results"])  # DataFrame con mÃ©tricas de todos los modelos
```

## ğŸ“¦ MÃ³dulos del Paquete

| MÃ³dulo | Funciones Principales |
|--------|----------------------|
| `preprocessing` | `split_data()`, `scale_features()`, `encode_categorical()`, `split_timeseries()` |
| `balancing` | `apply_smote()`, `undersample()`, `apply_combined()`, `check_imbalance()` |
| `metrics` | `classification_metrics()`, `regression_metrics()`, `timeseries_metrics()` |
| `validation` | `kfold_validation()`, `stratified_kfold()`, `manual_kfold()` |
| `threshold` | `apply_threshold()`, `optimize_threshold()`, `threshold_analysis()` |
| `models_classification` | `get_classification_models()`, `predict_classification()` |
| `models_regression` | `get_regression_models()` |
| `models_timeseries` | `HoltWintersModel`, `ARIMAModel`, `LSTMModel`, `get_timeseries_models()` |
| `benchmarking` | `run_benchmark()`, `rank_models()`, `benchmark_classification()` |

## ğŸ¤– Modelos Implementados

### ClasificaciÃ³n
- Logistic Regression, Random Forest, Decision Tree
- SVM (RBF), K-Nearest Neighbors, Naive Bayes
- Gradient Boosting, XGBoost (opcional)

### RegresiÃ³n
- Ridge, Lasso, Random Forest, Decision Tree
- SVR, K-Nearest Neighbors, Gradient Boosting, XGBoost

### Series de Tiempo
- Holt-Winters (estÃ¡ndar y calibrado)
- ARIMA(1,1,1) y ARIMA calibrado (bÃºsqueda automÃ¡tica de orden)
- LSTM (red neuronal recurrente)

## ğŸ“Š Datasets Integrados

| Dataset | Tipo | Muestras | Features |
|---------|------|----------|----------|
| Breast Cancer Wisconsin | ClasificaciÃ³n | 569 | 30 |
| Credit Card Fraud (Simulado) | ClasificaciÃ³n | 10,000 | 20 |
| California Housing | RegresiÃ³n | 20,640 | 8 |
| Airline Passengers | Series de Tiempo | 144 | â€” |

## ğŸ“– Referencias

- Hastie et al. (2009). The Elements of Statistical Learning.
- Chawla et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique.
- Box et al. (2015). Time Series Analysis: Forecasting and Control.
- Hochreiter & Schmidhuber (1997). Long Short-Term Memory.
